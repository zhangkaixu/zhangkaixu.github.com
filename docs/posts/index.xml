<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on 淺斟低唱</title>
    <link>https://zhangkaixu.github.io/posts/</link>
    <description>Recent content in Posts on 淺斟低唱</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Fri, 14 Apr 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://zhangkaixu.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ChatGPT或成为人工智能CPU的“加法器”</title>
      <link>https://zhangkaixu.github.io/posts/2023-04-14/</link>
      <pubDate>Fri, 14 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>https://zhangkaixu.github.io/posts/2023-04-14/</guid>
      <description>ChatGPT本身只是一个用于对话的语言模型，而且记忆有限。它本身并不能独立完成很复杂的任务。
但是，聊天的场景，也有它的优势之处：
使用自然语言作为输入输出，简直不是“可解释性”，而是“自解释性”。 输入输出非常灵活，没有所谓的“接口协议”的限制，它可以用来充当相当多的零部件。 现在的AutoGPT，就是反复调用ChatGPT，来完成更复杂的任务，从而形成一个更复杂的人工智能系统。
斯坦福的论文《generative agents interactive simulacra of human behavior》也给ChatGPT提供了类似内存的记忆模块。
回想日本上世纪搞的第五代计算机，这次似乎真的要搞出一个非冯诺依曼结构的“计算机”了。 那么像ChatGPT这种语言模型，会不会就成为如同现在CPU中“加法器”这样的最基础的运算单元呢？
虽然现在ChatGPT运算一次相当的缓慢且昂贵。 但谁敢说未来这玩意儿不会像集成电路一样的普及呢。 经济学家说不定会用GPU耗电量（占比）来衡量一个经济体的发达程度。</description>
    </item>
    
  </channel>
</rss>
